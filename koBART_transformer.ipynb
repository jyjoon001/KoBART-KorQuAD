{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "koBART-transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiwRG9R40zL2Z3JLKTrvK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyjoon001/KoBART-KorQuAD/blob/main/koBART_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ3EQosWUvi2",
        "outputId": "feeba072-d1cd-4bb0-9a14-09449b3afbfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/KoBERT-KorQuAD-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/KoBERT-KorQuAD-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xmfK-b0jfDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8343365-2cce-4a42-9281-280f32dccfcc"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install kobart-transformers\n",
        "from kobart_transformers import get_kobart_tokenizer\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting kobart-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/66/41/218da0e05f10b2ead251b6344bf76c725054ab2a6f5d17886e1ba0d96e35/kobart_transformers-0.1.4-py3-none-any.whl\n",
            "Installing collected packages: kobart-transformers\n",
            "Successfully installed kobart-transformers-0.1.4\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 18.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gZoYQk0BFAe",
        "outputId": "19693409-1166-4ce0-b334-bf12dd8a933d"
      },
      "source": [
        "!python3 /content/drive/MyDrive/KoBERT-KorQuAD-master/run_qa2.py --model_type kobert \\\n",
        "                       --model_name_or_path monologg/kobert \\\n",
        "                       --output_dir models2 \\\n",
        "                       --data_dir data \\\n",
        "                       --train_file KorQuAD_v1.0_train.json \\\n",
        "                       --predict_file KorQuAD_v1.0_dev.json \\\n",
        "                       --evaluate_during_training \\\n",
        "                       --per_gpu_train_batch_size 8 \\\n",
        "                       --per_gpu_eval_batch_size 8 \\\n",
        "                       --max_seq_length 512 \\\n",
        "                       --logging_steps 5000 \\\n",
        "                       --save_steps 400000 \\\n",
        "                       --do_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 03:46:42.337665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "05/03/2021 03:46:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639482640 acquired on /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e.lock\n",
            "[INFO|file_utils.py:1426] 2021-05-03 03:46:44,686 >> https://huggingface.co/monologg/kobert/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkc07e7xu\n",
            "Downloading: 100% 426/426 [00:00<00:00, 604kB/s]\n",
            "[INFO|file_utils.py:1430] 2021-05-03 03:46:44,703 >> storing https://huggingface.co/monologg/kobert/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "[INFO|file_utils.py:1433] 2021-05-03 03:46:44,703 >> creating metadata file for /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639482640 released on /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e.lock\n",
            "[INFO|configuration_utils.py:491] 2021-05-03 03:46:44,703 >> loading configuration file https://huggingface.co/monologg/kobert/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
            "[INFO|configuration_utils.py:527] 2021-05-03 03:46:44,704 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8002\n",
            "}\n",
            "\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639461584 acquired on /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f.lock\n",
            "[INFO|file_utils.py:1426] 2021-05-03 03:46:44,719 >> https://huggingface.co/monologg/kobert/resolve/main/tokenizer_78b3253a26.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl2go88eo\n",
            "Downloading: 100% 371k/371k [00:00<00:00, 32.4MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-05-03 03:46:44,748 >> storing https://huggingface.co/monologg/kobert/resolve/main/tokenizer_78b3253a26.model in cache at /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f\n",
            "[INFO|file_utils.py:1433] 2021-05-03 03:46:44,748 >> creating metadata file for /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639461584 released on /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f.lock\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639461584 acquired on /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f.lock\n",
            "[INFO|file_utils.py:1426] 2021-05-03 03:46:44,764 >> https://huggingface.co/monologg/kobert/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqmbzswwj\n",
            "Downloading: 100% 77.8k/77.8k [00:00<00:00, 14.8MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-05-03 03:46:44,786 >> storing https://huggingface.co/monologg/kobert/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f\n",
            "[INFO|file_utils.py:1433] 2021-05-03 03:46:44,786 >> creating metadata file for /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639461584 released on /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f.lock\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639445136 acquired on /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1.lock\n",
            "[INFO|file_utils.py:1426] 2021-05-03 03:46:44,830 >> https://huggingface.co/monologg/kobert/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7s3g6apj\n",
            "Downloading: 100% 51.0/51.0 [00:00<00:00, 57.7kB/s]\n",
            "[INFO|file_utils.py:1430] 2021-05-03 03:46:44,850 >> storing https://huggingface.co/monologg/kobert/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
            "[INFO|file_utils.py:1433] 2021-05-03 03:46:44,850 >> creating metadata file for /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490639445136 released on /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1.lock\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/tokenizer_78b3253a26.model from cache at /root/.cache/huggingface/transformers/7e55d7972628e6fc1babc614b5dd8bb43ab4f9d8541adc9fb1851112a7a7c5cc.4d2f4af7c2ca9df5b147978a95d38840e84801a378eee25756b008638e0bdc7f\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1\n",
            "[INFO|tokenization_utils_base.py:1707] 2021-05-03 03:46:44,867 >> loading file https://huggingface.co/monologg/kobert/resolve/main/tokenizer.json from cache at None\n",
            "05/03/2021 03:46:44 - INFO - filelock -   Lock 140490499681936 acquired on /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585.lock\n",
            "[INFO|file_utils.py:1426] 2021-05-03 03:46:44,900 >> https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptp_1ng3q\n",
            "Downloading: 100% 369M/369M [00:07<00:00, 49.3MB/s]\n",
            "[INFO|file_utils.py:1430] 2021-05-03 03:46:52,678 >> storing https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "[INFO|file_utils.py:1433] 2021-05-03 03:46:52,678 >> creating metadata file for /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "05/03/2021 03:46:52 - INFO - filelock -   Lock 140490499681936 released on /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585.lock\n",
            "[INFO|modeling_utils.py:1052] 2021-05-03 03:46:52,678 >> loading weights file https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
            "[INFO|modeling_utils.py:1168] 2021-05-03 03:46:55,264 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "[WARNING|modeling_utils.py:1171] 2021-05-03 03:46:55,264 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "05/03/2021 03:47:05 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=5e-05, local_rank=-1, logging_steps=5000, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=512, max_steps=-1, model_name_or_path='monologg/kobert', model_type='kobert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=10.0, output_dir='models2', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, predict_file='KorQuAD_v1.0_dev.json', save_steps=400000, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='KorQuAD_v1.0_train.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "05/03/2021 03:47:05 - INFO - __main__ -   Creating features from dataset file at data\n",
            "100% 1420/1420 [00:16<00:00, 88.54it/s] \n",
            "convert squad examples to features:  23% 13729/60407 [01:34<05:20, 145.49it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "0MpQU69MUw4S",
        "outputId": "70bd1f44-791b-4ce3-bed9-8347f0e77486"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (BartForConditionalGeneration,\n",
        "                          PreTrainedTokenizerFast)\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-41a87a09dbca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloggers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl_loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-_lHI31U7BV"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='KoBART-KorQuAD')\n",
        "\n",
        "parser.add_argument('--checkpoint_path',\n",
        "                    type=str,\n",
        "                    help='checkpoint path')\n",
        "\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "class ArgsBase():\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = argparse.ArgumentParser(\n",
        "            parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--train_file',\n",
        "                            type=str,\n",
        "                            default='Chatbot_data/train.csv',\n",
        "                            help='train file')\n",
        "\n",
        "        parser.add_argument('--test_file',\n",
        "                            type=str,\n",
        "                            default='Chatbot_data/test.csv',\n",
        "                            help='test file')\n",
        "\n",
        "        parser.add_argument('--tokenizer_path',\n",
        "                            type=str,\n",
        "                            default='tokenizer',\n",
        "                            help='tokenizer')\n",
        "        parser.add_argument('--batch_size',\n",
        "                            type=int,\n",
        "                            default=14,\n",
        "                            help='')\n",
        "        parser.add_argument('--max_seq_len',\n",
        "                            type=int,\n",
        "                            default=36,\n",
        "                            help='max seq len')\n",
        "        return parser\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, filepath, tok_vocab, max_seq_len=128) -> None:\n",
        "        self.filepath = filepath\n",
        "        self.data = pd.read_csv(self.filepath)\n",
        "        self.bos_token = '<s>'\n",
        "        self.eos_token = '</s>'\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.tokenizer = PreTrainedTokenizerFast(\n",
        "            tokenizer_file=tok_vocab,\n",
        "            bos_token=self.bos_token, eos_token=self.eos_token, unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def make_input_id_mask(self, tokens, index):\n",
        "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_id)\n",
        "        if len(input_id) < self.max_seq_len:\n",
        "            while len(input_id) < self.max_seq_len:\n",
        "                input_id += [self.tokenizer.pad_token_id]\n",
        "                attention_mask += [0]\n",
        "        else:\n",
        "            # logging.warning(f'exceed max_seq_len for given article : {index}')\n",
        "            input_id = input_id[:self.max_seq_len - 1] + [\n",
        "                self.tokenizer.eos_token_id]\n",
        "            attention_mask = attention_mask[:self.max_seq_len]\n",
        "        return input_id, attention_mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record = self.data.iloc[index]\n",
        "        q, a = record['Q'], record['A']\n",
        "        q_tokens = [self.bos_token] + \\\n",
        "            self.tokenizer.tokenize(q) + [self.eos_token]\n",
        "        a_tokens = [self.bos_token] + \\\n",
        "            self.tokenizer.tokenize(a) + [self.eos_token]\n",
        "        encoder_input_id, encoder_attention_mask = self.make_input_id_mask(\n",
        "            q_tokens, index)\n",
        "        decoder_input_id, decoder_attention_mask = self.make_input_id_mask(\n",
        "            a_tokens, index)\n",
        "        labels = self.tokenizer.convert_tokens_to_ids(\n",
        "            a_tokens[1:(self.max_seq_len + 1)])\n",
        "        if len(labels) < self.max_seq_len:\n",
        "            while len(labels) < self.max_seq_len:\n",
        "                # for cross entropy loss masking\n",
        "                labels += [-100]\n",
        "        return {'input_ids': np.array(encoder_input_id, dtype=np.int_),\n",
        "                'attention_mask': np.array(encoder_attention_mask, dtype=np.float_),\n",
        "                'decoder_input_ids': np.array(decoder_input_id, dtype=np.int_),\n",
        "                'decoder_attention_mask': np.array(decoder_attention_mask, dtype=np.float_),\n",
        "                'labels': np.array(labels, dtype=np.int_)}\n",
        "\n",
        "\n",
        "class ChatDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_file,\n",
        "                 test_file, tok_vocab,\n",
        "                 max_seq_len=128,\n",
        "                 batch_size=32,\n",
        "                 num_workers=5):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.train_file_path = train_file\n",
        "        self.test_file_path = test_file\n",
        "        self.tok_vocab = tok_vocab\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = argparse.ArgumentParser(\n",
        "            parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--num_workers',\n",
        "                            type=int,\n",
        "                            default=5,\n",
        "                            help='num of worker for dataloader')\n",
        "        return parser\n",
        "\n",
        "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
        "    def setup(self, stage):\n",
        "        # split dataset\n",
        "        self.train = ChatDataset(self.train_file_path,\n",
        "                                 self.tok_vocab,\n",
        "                                 self.max_seq_len)\n",
        "        self.test = ChatDataset(self.test_file_path,\n",
        "                                self.tok_vocab,\n",
        "                                self.max_seq_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train = DataLoader(self.train,\n",
        "                           batch_size=self.batch_size,\n",
        "                           num_workers=self.num_workers, shuffle=True)\n",
        "        return train\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val = DataLoader(self.test,\n",
        "                         batch_size=self.batch_size,\n",
        "                         num_workers=self.num_workers, shuffle=False)\n",
        "        return val\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test = DataLoader(self.test,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers=self.num_workers, shuffle=False)\n",
        "        return test\n",
        "\n",
        "\n",
        "class Base(pl.LightningModule):\n",
        "    def __init__(self, hparams, **kwargs) -> None:\n",
        "        super(Base, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        # add model specific args\n",
        "        parser = argparse.ArgumentParser(\n",
        "            parents=[parent_parser], add_help=False)\n",
        "\n",
        "        parser.add_argument('--batch-size',\n",
        "                            type=int,\n",
        "                            default=14,\n",
        "                            help='batch size for training (default: 96)')\n",
        "\n",
        "        parser.add_argument('--lr',\n",
        "                            type=float,\n",
        "                            default=5e-5,\n",
        "                            help='The initial learning rate')\n",
        "\n",
        "        parser.add_argument('--warmup_ratio',\n",
        "                            type=float,\n",
        "                            default=0.1,\n",
        "                            help='warmup ratio')\n",
        "\n",
        "        parser.add_argument('--model_path',\n",
        "                            type=str,\n",
        "                            default=None,\n",
        "                            help='kobart model path')\n",
        "        return parser\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(\n",
        "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.lr, correct_bias=False)\n",
        "        # warm up lr\n",
        "        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n",
        "        data_len = len(self.train_dataloader().dataset)\n",
        "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
        "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
        "        logging.info(f'num_train_steps : {num_train_steps}')\n",
        "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
        "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "        lr_scheduler = {'scheduler': scheduler, \n",
        "                        'monitor': 'loss', 'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "\n",
        "class KoBARTConditionalGeneration(Base):\n",
        "    def __init__(self, hparams, **kwargs):\n",
        "        super(KoBARTConditionalGeneration, self).__init__(hparams, **kwargs)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.hparams.model_path)\n",
        "        self.model.train()\n",
        "        self.bos_token = '<s>'\n",
        "        self.eos_token = '</s>'\n",
        "        self.tokenizer = PreTrainedTokenizerFast(\n",
        "            tokenizer_file=os.path.join(self.hparams.tokenizer_path, 'model.json'),\n",
        "            bos_token=self.bos_token, eos_token=self.eos_token, unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.model(input_ids=inputs['input_ids'],\n",
        "                          attention_mask=inputs['attention_mask'],\n",
        "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
        "                          decoder_attention_mask=inputs['decoder_attention_mask'],\n",
        "                          labels=inputs['labels'], return_dict=True)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outs = self(batch)\n",
        "        loss = outs.loss\n",
        "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outs = self(batch)\n",
        "        loss = outs['loss']\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "\n",
        "    def chat(self, text):\n",
        "        input_ids =  [self.tokenizer.bos_token_id] + self.tokenizer.encode(text) + [self.tokenizer.eos_token_id]\n",
        "        res_ids = self.model.generate(torch.tensor([input_ids]),\n",
        "                                            max_length=self.hparams.max_seq_len,\n",
        "                                            num_beams=5,\n",
        "                                            eos_token_id=self.tokenizer.eos_token_id,\n",
        "                                            bad_words_ids=[[self.tokenizer.unk_token_id]])        \n",
        "        a = self.tokenizer.batch_decode(res_ids.tolist())[0]\n",
        "        return a.replace('<s>', '').replace('</s>', '')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = Base.add_model_specific_args(parser)\n",
        "    parser = ArgsBase.add_model_specific_args(parser)\n",
        "    parser = ChatDataModule.add_model_specific_args(parser)\n",
        "    parser = pl.Trainer.add_argparse_args(parser)\n",
        "    args = parser.parse_args()\n",
        "    logging.info(args)\n",
        "\n",
        "    model = KoBARTConditionalGeneration(args)\n",
        "\n",
        "    dm = ChatDataModule(args.train_file,\n",
        "                        args.test_file,\n",
        "                        os.path.join(args.tokenizer_path, 'model.json'),\n",
        "                        max_seq_len=args.max_seq_len,\n",
        "                        num_workers=args.num_workers)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss',\n",
        "                                                       dirpath=args.default_root_dir,\n",
        "                                                       filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
        "                                                       verbose=True,\n",
        "                                                       save_last=True,\n",
        "                                                       mode='min',\n",
        "                                                       save_top_k=-1,\n",
        "                                                       prefix='kobart_chitchat')\n",
        "    tb_logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir, 'tb_logs'))\n",
        "    lr_logger = pl.callbacks.LearningRateMonitor()\n",
        "    trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger,\n",
        "                                            callbacks=[checkpoint_callback, lr_logger])\n",
        "    trainer.fit(model, dm)\n",
        "    if args.chat:\n",
        "        model.model.eval()\n",
        "        while 1:\n",
        "            q = input('user > ').strip()\n",
        "            if q == 'quit':\n",
        "                break\n",
        "            print(\"Simsimi > {}\".format(model.chat(q)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9bsr75bEL8J",
        "outputId": "462c041c-db3e-4efa-ad71-9339517f2a61"
      },
      "source": [
        "from kobart import get_kobart_tokenizer\n",
        "kobart_tokenizer = get_kobart_tokenizer()\n",
        "kobart_tokenizer.tokenize(\"안녕하세요. 한국어 BART 입니다.🤣:)l^o\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁안녕하', '세요.', '▁한국어', '▁B', 'A', 'R', 'T', '▁입', '니다.', '🤣', ':)', 'l^o']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dn4_LekGemj",
        "outputId": "b9854841-e859-4e59-905c-9a5a8ac2d168"
      },
      "source": [
        "from transformers import BartModel\n",
        "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
        "kobart_tokenizer = get_kobart_tokenizer()\n",
        "model = BartModel.from_pretrained(get_pytorch_kobart_model())\n",
        "inputs = kobart_tokenizer(['안녕하세요.'], return_tensors='pt')\n",
        "model(inputs['input_ids'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqModelOutput([('last_hidden_state',\n",
              "                     tensor([[[-0.4418, -4.3673,  3.2404,  ...,  5.8832,  4.0629,  3.5540],\n",
              "                              [-0.1316, -4.6446,  2.5955,  ...,  6.0093,  2.7467,  3.0007]]],\n",
              "                            grad_fn=<NativeLayerNormBackward>)),\n",
              "                    ('past_key_values',\n",
              "                     ((tensor([[[[-9.7980e-02, -6.6584e-01, -1.8089e+00,  ...,  9.6023e-01,\n",
              "                                  -1.8818e-01, -1.3252e+00],\n",
              "                                 [-6.2507e-01,  5.1009e-01, -7.4878e-01,  ...,  8.6230e-01,\n",
              "                                   1.5722e-01, -6.0267e-01]],\n",
              "                       \n",
              "                                [[ 5.4597e-01, -2.3990e-01,  1.5901e+00,  ...,  4.3655e-01,\n",
              "                                   7.9514e-01,  8.9880e-02],\n",
              "                                 [-1.7327e-01, -6.3167e-01,  4.5152e-02,  ..., -1.4111e-01,\n",
              "                                   1.8678e-01, -1.2081e-01]],\n",
              "                       \n",
              "                                [[ 1.4621e+00,  1.8980e+00, -7.6696e-01,  ...,  1.5695e+00,\n",
              "                                   6.7921e-02, -3.9372e-01],\n",
              "                                 [-4.1204e-02,  1.7132e+00, -1.1863e+00,  ..., -2.2272e-01,\n",
              "                                   9.8309e-02,  8.1729e-01]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 4.8868e-01,  1.2633e+00, -1.1658e-01,  ..., -3.1989e-01,\n",
              "                                   1.2202e+00, -7.9021e-02],\n",
              "                                 [-8.4946e-01,  8.9379e-02, -1.0224e+00,  ...,  3.3125e-01,\n",
              "                                  -2.5262e-01,  5.0875e-01]],\n",
              "                       \n",
              "                                [[ 1.5854e+00,  3.2461e-01,  3.0826e+00,  ..., -1.6728e+00,\n",
              "                                   1.2071e+00, -3.5671e-01],\n",
              "                                 [ 5.5400e-02, -9.2782e-01, -2.3055e-03,  ..., -6.1646e-01,\n",
              "                                   1.0880e+00,  2.8645e-01]],\n",
              "                       \n",
              "                                [[ 8.7081e-01, -4.6088e-01, -2.8388e+00,  ...,  1.6038e+00,\n",
              "                                  -1.0963e+00, -1.8732e-01],\n",
              "                                 [ 4.5471e-01, -3.1087e-02, -2.4484e+00,  ...,  1.9392e+00,\n",
              "                                  -4.0694e-01, -1.9906e-01]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.2187,  1.3322, -0.0016,  ..., -0.1200, -0.0395,  0.0971],\n",
              "                                 [-0.2722, -0.0590,  0.4620,  ...,  0.1822, -0.0171, -0.2176]],\n",
              "                       \n",
              "                                [[ 0.1439,  0.0074,  0.0249,  ...,  0.2148, -0.5016,  0.1263],\n",
              "                                 [ 0.3625, -0.3192, -0.2254,  ...,  0.6312,  0.1061,  0.3506]],\n",
              "                       \n",
              "                                [[-0.0862, -0.0073, -0.0479,  ...,  0.0110,  0.0198,  0.1909],\n",
              "                                 [-0.4622, -0.4537, -0.6060,  ...,  0.5620,  1.3319,  0.0989]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.0868, -0.1329,  0.1862,  ..., -0.0489, -0.0084,  0.0151],\n",
              "                                 [-0.4065,  0.4082,  0.7682,  ..., -0.0909,  0.1701,  0.0145]],\n",
              "                       \n",
              "                                [[ 0.1256,  0.1170,  0.0912,  ...,  0.0623, -0.0574,  0.0611],\n",
              "                                 [-0.2664, -0.4485,  0.1439,  ...,  0.2324,  0.3832, -1.0943]],\n",
              "                       \n",
              "                                [[ 0.0702,  0.0227,  0.0311,  ..., -0.0203,  0.0602,  0.0033],\n",
              "                                 [-0.3197,  0.4879,  0.1972,  ..., -0.3241,  0.1762,  0.5655]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-1.0648, -2.5643, -1.2208,  ...,  1.4609,  1.0088,  0.8616],\n",
              "                                 [-1.3031, -2.4746, -1.2874,  ...,  1.4641,  0.6007,  0.5663]],\n",
              "                       \n",
              "                                [[-0.9957, -1.4633, -0.6104,  ..., -0.0807,  0.6639,  1.1657],\n",
              "                                 [-0.8524, -2.1023, -0.3089,  ...,  0.1747,  0.9409,  0.8996]],\n",
              "                       \n",
              "                                [[ 1.7265, -0.0293, -0.3755,  ..., -0.6951,  0.4730,  0.3141],\n",
              "                                 [ 1.5839, -0.6262, -0.5320,  ...,  0.2295,  0.5922,  0.6125]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.1965, -0.4015,  1.1312,  ..., -0.5016, -0.1073,  0.0549],\n",
              "                                 [-0.5866, -0.6739,  1.0961,  ..., -0.3082, -0.1296,  0.0126]],\n",
              "                       \n",
              "                                [[-0.0225,  0.6815, -1.6006,  ...,  1.7747, -0.4324, -1.2341],\n",
              "                                 [ 0.2012,  0.2133, -2.0224,  ...,  1.0445, -0.3429, -1.1153]],\n",
              "                       \n",
              "                                [[-0.4898, -0.2916,  0.1036,  ..., -1.1657, -2.0047, -0.5035],\n",
              "                                 [ 0.2014,  0.0172, -0.2052,  ..., -1.1187, -2.9760, -0.2174]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.2822, -0.1429, -0.0214,  ...,  0.0815,  0.1900, -0.2706],\n",
              "                                 [ 0.0435, -0.3802, -0.3064,  ...,  0.5159,  0.1527, -0.2039]],\n",
              "                       \n",
              "                                [[-0.8001,  0.0466,  1.0855,  ..., -0.2635, -0.2926,  0.3927],\n",
              "                                 [-0.3446, -0.6481,  0.5623,  ...,  0.3295,  0.6792,  0.4060]],\n",
              "                       \n",
              "                                [[-0.8864, -0.0462, -0.4937,  ..., -1.2180,  0.5027,  0.1991],\n",
              "                                 [-0.8240, -0.4198,  0.1187,  ..., -0.3042,  0.6595, -0.1653]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.3499,  0.7639,  0.0130,  ...,  0.3620,  0.4275, -0.6412],\n",
              "                                 [-0.1910,  0.5114, -0.0352,  ...,  0.3060,  0.2019, -0.4844]],\n",
              "                       \n",
              "                                [[ 0.2417,  0.1567,  0.2082,  ...,  0.1640, -0.1767, -0.1738],\n",
              "                                 [ 0.1304, -0.0716, -0.1840,  ...,  0.3307,  0.1039,  0.1808]],\n",
              "                       \n",
              "                                [[-0.4749, -0.9211,  1.0276,  ..., -0.3349, -0.2668, -0.2143],\n",
              "                                 [ 0.0051, -0.7191,  0.6315,  ..., -0.2618,  0.0611, -0.1337]]]],\n",
              "                              grad_fn=<CopyBackwards>)),\n",
              "                      (tensor([[[[ 0.0568,  0.1593,  0.3356,  ..., -0.0771, -0.1446, -0.1562],\n",
              "                                 [ 1.6254, -1.5403,  1.2852,  ...,  0.3466, -1.0204, -1.6762]],\n",
              "                       \n",
              "                                [[ 0.2014, -0.2892, -0.4829,  ...,  0.2817, -0.1413,  1.5805],\n",
              "                                 [ 0.6554,  1.6124,  0.4492,  ...,  0.3675,  1.0584, -3.6320]],\n",
              "                       \n",
              "                                [[-0.1167,  0.6492, -0.5715,  ...,  0.0990, -1.3751, -0.3157],\n",
              "                                 [ 0.5295, -2.8552,  1.0669,  ...,  0.8807,  2.6310, -0.9037]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.0606,  0.1003,  0.1334,  ...,  0.0648,  0.0886,  1.1766],\n",
              "                                 [ 0.9016, -1.1139,  1.2164,  ...,  0.0885,  0.5449, -1.5874]],\n",
              "                       \n",
              "                                [[ 0.1461,  0.1861,  0.0330,  ..., -0.0966, -0.3876,  0.2000],\n",
              "                                 [-0.4877,  0.6808,  0.8141,  ...,  1.1511, -1.0541, -0.7516]],\n",
              "                       \n",
              "                                [[ 0.4198,  0.1532,  0.4183,  ..., -0.1995, -0.0194,  0.3649],\n",
              "                                 [ 1.9049,  0.4632, -1.9892,  ...,  0.2458,  0.5797, -0.5118]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 1.5379e-02,  4.6779e-02,  2.3209e-02,  ..., -3.6808e-03,\n",
              "                                  -6.6380e-02,  8.8301e-04],\n",
              "                                 [ 3.3722e-01,  5.9773e-01, -9.7631e-01,  ..., -6.6999e-01,\n",
              "                                  -8.6936e-01, -1.0744e+00]],\n",
              "                       \n",
              "                                [[-8.2674e-03, -2.8380e-02, -3.8986e-02,  ...,  1.9672e-02,\n",
              "                                   2.6157e-03,  2.0187e-03],\n",
              "                                 [ 4.2856e-01,  1.3611e+00,  4.8732e-01,  ..., -5.9339e-02,\n",
              "                                  -1.1926e-01, -4.2579e-02]],\n",
              "                       \n",
              "                                [[ 6.0655e-02, -1.8254e-02, -1.5229e-02,  ..., -1.6663e-02,\n",
              "                                   8.4880e-03, -4.8301e-02],\n",
              "                                 [-1.0497e-01, -8.1324e-02,  3.3243e-01,  ...,  3.5059e-01,\n",
              "                                  -7.1202e-02,  1.6473e-02]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 3.6912e-02, -6.7970e-02, -8.0440e-02,  ..., -9.4137e-02,\n",
              "                                   3.8143e-03,  7.8197e-02],\n",
              "                                 [-2.4057e-01,  2.6768e-03,  7.9480e-01,  ...,  2.9896e-01,\n",
              "                                   7.9629e-01,  9.2801e-01]],\n",
              "                       \n",
              "                                [[ 1.9845e-02,  4.9133e-03, -3.3145e-02,  ...,  7.2793e-02,\n",
              "                                   5.9300e-02, -5.7760e-02],\n",
              "                                 [-3.6504e-01,  5.4651e-01, -4.3529e-02,  ..., -3.2015e-01,\n",
              "                                   7.9082e-01, -9.7559e-02]],\n",
              "                       \n",
              "                                [[ 3.4483e-02, -1.2532e-02,  1.5040e-02,  ...,  1.7609e-02,\n",
              "                                  -2.3566e-02, -2.2959e-02],\n",
              "                                 [-1.1245e+00, -6.8237e-01,  1.0025e-01,  ..., -2.7405e-01,\n",
              "                                  -5.1046e-01, -1.9639e-01]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 2.3286e+00, -4.2951e-02,  3.0110e+00,  ...,  1.0627e+00,\n",
              "                                  -9.1236e-01,  2.6603e+00],\n",
              "                                 [ 3.0073e+00, -1.5674e-01,  2.8979e+00,  ...,  9.7826e-01,\n",
              "                                  -5.3163e-01,  3.0233e+00]],\n",
              "                       \n",
              "                                [[ 1.7130e+00, -2.9926e+00,  1.8776e+00,  ...,  7.5416e-01,\n",
              "                                  -8.7124e-01, -4.8129e-01],\n",
              "                                 [ 2.5121e+00, -3.2266e+00,  2.2682e+00,  ...,  4.4046e-01,\n",
              "                                  -5.1476e-01, -5.2341e-01]],\n",
              "                       \n",
              "                                [[-8.1486e-01, -9.5213e-01, -8.3259e-04,  ...,  1.5024e+00,\n",
              "                                  -5.7569e-01,  1.9130e-01],\n",
              "                                 [-8.1358e-01, -1.2385e+00,  9.3572e-01,  ...,  1.1406e+00,\n",
              "                                  -9.5044e-01, -4.8016e-01]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 8.1763e-01, -9.2276e-01,  1.5260e+00,  ...,  2.9562e-02,\n",
              "                                   1.5474e-02, -8.2440e-01],\n",
              "                                 [ 1.4520e-02, -1.5225e+00,  1.9413e+00,  ..., -3.6908e-01,\n",
              "                                   5.3651e-01, -2.1469e-01]],\n",
              "                       \n",
              "                                [[-1.0017e+00, -5.5066e-01, -1.8923e+00,  ..., -5.8160e-01,\n",
              "                                   4.2548e-01, -6.7959e-01],\n",
              "                                 [-5.8291e-01, -8.2588e-01, -2.0550e+00,  ..., -2.2966e-01,\n",
              "                                   2.4397e-02, -2.6139e-01]],\n",
              "                       \n",
              "                                [[-1.4829e+00, -8.8995e-01,  9.4026e-01,  ...,  1.4175e+00,\n",
              "                                  -3.9160e-01, -9.2237e-01],\n",
              "                                 [-1.0111e+00, -1.1450e+00,  1.1993e+00,  ...,  2.0617e+00,\n",
              "                                  -5.9144e-01, -1.3860e+00]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.1935, -0.1730,  0.3934,  ..., -0.0840,  0.2339,  0.1091],\n",
              "                                 [-0.0569, -0.3551,  0.3823,  ...,  0.0197, -0.0043, -0.0436]],\n",
              "                       \n",
              "                                [[ 0.2364,  0.4606,  0.5622,  ...,  0.4820,  0.1865,  0.6264],\n",
              "                                 [ 0.5973,  0.2811,  0.2784,  ...,  0.1489, -0.4533,  0.4468]],\n",
              "                       \n",
              "                                [[ 0.4116,  0.4325, -0.3306,  ..., -0.7509,  0.2645, -0.1624],\n",
              "                                 [ 0.8045,  0.3654, -0.4785,  ..., -0.7986, -0.0484, -0.1889]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.1706, -1.0074,  0.2970,  ...,  0.4921,  0.5458,  0.1225],\n",
              "                                 [ 0.3865, -1.1976, -0.2150,  ...,  0.6561,  0.4510,  0.1755]],\n",
              "                       \n",
              "                                [[ 0.0047, -0.7317,  0.3213,  ...,  0.5871, -0.2654, -0.4047],\n",
              "                                 [-0.1921, -0.4592,  0.0943,  ...,  0.3975, -0.2417, -0.3156]],\n",
              "                       \n",
              "                                [[-0.1002, -0.3230,  0.6674,  ..., -0.2355, -0.6963, -0.3525],\n",
              "                                 [-0.1739, -0.4883,  0.4101,  ..., -0.2514, -0.6257, -0.6858]]]],\n",
              "                              grad_fn=<CopyBackwards>)),\n",
              "                      (tensor([[[[-0.1980, -0.8728,  0.6663,  ...,  0.2215,  0.3865, -0.0239],\n",
              "                                 [ 0.7417,  3.5312, -0.8480,  ..., -0.0536, -0.7790, -0.2229]],\n",
              "                       \n",
              "                                [[ 0.3768, -0.2689, -0.0428,  ...,  0.1046, -0.1428, -0.9039],\n",
              "                                 [ 0.0478,  2.5229, -0.2297,  ..., -0.2463,  3.8656,  4.1838]],\n",
              "                       \n",
              "                                [[-0.0777,  0.5079,  0.0822,  ...,  0.0193, -0.5585,  0.1365],\n",
              "                                 [ 0.6972, -2.6046,  1.8231,  ..., -1.2138,  2.4488,  0.6540]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.4664,  0.2169,  0.0277,  ...,  1.1934,  0.6856,  0.1670],\n",
              "                                 [ 0.6041,  0.9994,  0.1424,  ..., -1.8193, -1.1613, -0.5548]],\n",
              "                       \n",
              "                                [[ 0.0759,  0.1255, -0.0151,  ...,  0.0307,  0.1052, -0.6455],\n",
              "                                 [ 0.6701,  0.7716,  0.2974,  ..., -0.7394,  0.5873,  3.0509]],\n",
              "                       \n",
              "                                [[ 0.1543,  0.2131, -0.0718,  ...,  0.0975, -0.0069, -0.3904],\n",
              "                                 [-0.5077, -1.8357,  2.1244,  ...,  0.3969,  1.3626, -0.0225]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 2.9269e-02,  1.1764e-01, -1.2732e-02,  ..., -8.6454e-02,\n",
              "                                   1.2252e-01,  6.7036e-03],\n",
              "                                 [ 3.9763e-01, -2.9862e-01, -4.3627e-01,  ..., -5.6594e-02,\n",
              "                                  -2.9131e-01, -6.3776e-01]],\n",
              "                       \n",
              "                                [[ 1.6335e-02, -2.4424e-02, -4.8432e-03,  ..., -5.6251e-02,\n",
              "                                  -6.8835e-02,  1.2608e-02],\n",
              "                                 [ 3.5408e-01,  6.1424e-01,  3.5653e-01,  ..., -3.0891e-02,\n",
              "                                   7.4476e-02,  2.0799e-02]],\n",
              "                       \n",
              "                                [[ 2.5243e-02,  1.0407e-02, -4.8344e-02,  ...,  6.4945e-03,\n",
              "                                   3.9794e-04,  1.2100e-01],\n",
              "                                 [ 7.1725e-01, -1.8262e-01, -3.0229e-01,  ..., -4.4057e-01,\n",
              "                                  -3.8583e-02, -2.3157e-01]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 9.2560e-03,  1.4655e-02,  3.4054e-02,  ...,  9.6428e-03,\n",
              "                                  -6.3913e-03,  2.7569e-02],\n",
              "                                 [ 4.2214e-01,  3.7137e-01,  2.3890e-01,  ..., -1.7631e-01,\n",
              "                                   9.0582e-01,  1.0196e-01]],\n",
              "                       \n",
              "                                [[ 3.1055e-03,  1.0484e-02, -4.7095e-03,  ...,  4.8638e-02,\n",
              "                                  -1.5250e-02, -7.0406e-02],\n",
              "                                 [ 9.7427e-02,  2.6014e-01,  3.1176e-01,  ..., -4.8579e-01,\n",
              "                                   9.3071e-01, -1.1125e-01]],\n",
              "                       \n",
              "                                [[ 2.8621e-02, -2.8968e-03, -3.1328e-02,  ...,  2.5224e-02,\n",
              "                                   1.8680e-02, -4.3578e-02],\n",
              "                                 [ 3.0150e-01, -1.9468e-01, -1.7805e-01,  ...,  5.1854e-01,\n",
              "                                  -3.4874e-02,  9.8429e-02]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-1.7399,  1.5544,  0.8420,  ...,  0.1949,  0.3739, -0.4898],\n",
              "                                 [-1.6240,  1.3219,  0.9978,  ..., -0.4160, -0.1783,  0.3793]],\n",
              "                       \n",
              "                                [[-0.2877, -0.3362, -0.3745,  ...,  0.3365,  0.1496, -1.2037],\n",
              "                                 [-0.2730, -0.1071, -0.1413,  ...,  0.3996, -0.1407, -1.1605]],\n",
              "                       \n",
              "                                [[ 1.4108, -0.6566,  1.7061,  ...,  1.4992,  0.5674, -0.9317],\n",
              "                                 [ 1.7853, -1.0445,  1.7326,  ...,  1.7643,  0.5169, -1.0316]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-1.7319,  3.2136,  1.1013,  ..., -0.8693, -0.2972, -1.2679],\n",
              "                                 [-2.0636,  3.6187,  0.9261,  ..., -0.9856, -0.3694, -1.0883]],\n",
              "                       \n",
              "                                [[-0.9236,  0.7800, -0.0765,  ...,  1.6758,  1.2948,  0.3508],\n",
              "                                 [-0.6022,  0.7458, -0.1418,  ...,  2.1779,  1.1909,  0.1315]],\n",
              "                       \n",
              "                                [[ 0.4240,  0.0391, -1.3894,  ..., -1.0823, -0.4126,  0.9783],\n",
              "                                 [ 0.5731, -0.2723, -1.8803,  ..., -1.2079, -1.0119,  0.5113]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.0985, -0.6352, -0.0868,  ...,  0.0363,  0.0543, -0.3238],\n",
              "                                 [-0.2378, -0.1665, -0.3516,  ...,  0.3921, -0.2744, -0.3039]],\n",
              "                       \n",
              "                                [[ 0.8450,  0.2317,  0.6335,  ...,  0.5318, -0.0949,  0.1750],\n",
              "                                 [ 0.5121,  0.0876,  0.5792,  ..., -0.0148, -0.4605,  0.2020]],\n",
              "                       \n",
              "                                [[-0.2443, -0.0378,  0.2317,  ...,  0.0219, -0.1216,  0.0395],\n",
              "                                 [-0.4156,  0.0576,  0.1678,  ...,  0.0501, -0.3476, -0.2093]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.1073,  0.2830,  0.1495,  ..., -1.0262,  0.5416,  0.5043],\n",
              "                                 [ 0.2258,  0.0643, -0.1481,  ..., -1.0380,  0.3420,  0.5458]],\n",
              "                       \n",
              "                                [[-0.1796, -0.4815,  0.2412,  ..., -0.1129,  0.5764,  0.0777],\n",
              "                                 [-0.2803, -0.4700,  0.0848,  ...,  0.2658,  0.4753,  0.1039]],\n",
              "                       \n",
              "                                [[ 0.5494,  0.6024, -0.6296,  ...,  0.1007,  0.8262, -0.1736],\n",
              "                                 [ 0.3254,  0.5532, -0.1605,  ..., -0.2396,  0.2754,  0.1460]]]],\n",
              "                              grad_fn=<CopyBackwards>)),\n",
              "                      (tensor([[[[ 0.8659,  1.2230, -0.0036,  ..., -0.1768,  1.2656,  1.3975],\n",
              "                                 [-2.4391, -1.6445,  1.7410,  ..., -1.8302, -2.0103, -1.7548]],\n",
              "                       \n",
              "                                [[ 0.4183,  0.5536,  0.7881,  ..., -0.2234,  0.4811, -0.0375],\n",
              "                                 [-1.6890, -0.3787, -1.0577,  ..., -0.8861, -1.5151, -0.2259]],\n",
              "                       \n",
              "                                [[-0.5553, -1.0096,  0.1065,  ..., -0.4831,  1.4737, -0.0735],\n",
              "                                 [ 0.7138,  0.3943,  0.3157,  ...,  0.2931, -0.2829,  0.3843]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.6875,  0.4129, -0.7369,  ..., -1.1018, -0.7937,  0.5081],\n",
              "                                 [ 1.8270,  0.4463, -1.1159,  ...,  0.7450,  1.0975,  0.1329]],\n",
              "                       \n",
              "                                [[ 0.2528,  0.2051,  0.2412,  ...,  0.1542,  0.5065, -0.0168],\n",
              "                                 [ 0.0560,  1.4332,  0.3786,  ...,  0.1777, -0.5803,  0.0421]],\n",
              "                       \n",
              "                                [[-0.2057,  0.4577,  0.8795,  ..., -0.3249, -0.0460,  0.9419],\n",
              "                                 [-1.3328,  0.4214, -0.5938,  ..., -0.5810,  0.0759, -0.4099]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-4.0518e-02, -5.9250e-02, -1.6183e-02,  ..., -8.3794e-04,\n",
              "                                  -3.3078e-02,  7.6099e-02],\n",
              "                                 [-4.3446e-01, -1.9747e-01, -4.3507e-01,  ..., -4.8681e-01,\n",
              "                                  -2.7235e-01, -8.4327e-02]],\n",
              "                       \n",
              "                                [[-1.4299e-01,  1.0976e-01, -3.2061e-02,  ..., -7.9136e-02,\n",
              "                                  -6.2019e-02,  1.1654e-03],\n",
              "                                 [ 5.5592e-01, -4.1033e-02,  1.5684e-01,  ...,  4.7759e-01,\n",
              "                                   4.6287e-01,  1.3250e-01]],\n",
              "                       \n",
              "                                [[-1.0311e-02, -3.4038e-02, -1.5415e-01,  ...,  2.3261e-02,\n",
              "                                  -1.4703e-02,  2.4091e-02],\n",
              "                                 [-8.1498e-01, -3.5524e-01,  3.3233e-02,  ...,  2.5945e-01,\n",
              "                                   5.5901e-02,  1.0831e+00]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-1.8605e-02,  5.5119e-02,  2.7359e-03,  ..., -2.4560e-02,\n",
              "                                   6.2882e-03,  5.5262e-03],\n",
              "                                 [ 1.2290e-01, -4.1711e-01, -3.5411e-02,  ..., -2.1633e-01,\n",
              "                                  -4.8053e-01,  9.0854e-02]],\n",
              "                       \n",
              "                                [[-8.3878e-04,  6.2181e-02, -3.9761e-02,  ...,  5.1470e-02,\n",
              "                                   7.0819e-02,  8.0597e-03],\n",
              "                                 [ 5.9444e-02,  5.7503e-01, -7.6640e-01,  ...,  4.2504e-03,\n",
              "                                  -7.7810e-02,  9.4069e-02]],\n",
              "                       \n",
              "                                [[ 7.7253e-02,  7.0009e-02, -1.6194e-02,  ...,  2.5476e-02,\n",
              "                                  -8.1933e-03,  2.2595e-02],\n",
              "                                 [ 1.1499e-01, -3.9242e-01,  7.6922e-01,  ...,  2.9296e-01,\n",
              "                                   8.2074e-01,  6.4681e-01]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 2.0849, -0.0636,  0.5000,  ...,  0.9966,  1.7627,  1.5514],\n",
              "                                 [ 1.7876, -0.7635,  1.6988,  ...,  2.0722,  1.4739,  1.1660]],\n",
              "                       \n",
              "                                [[-0.8371,  0.9337,  0.7837,  ...,  0.0498,  1.6516,  0.1707],\n",
              "                                 [-0.5368,  1.0880,  0.5653,  ..., -0.0634,  1.3486, -0.6582]],\n",
              "                       \n",
              "                                [[-0.0912,  0.6096, -0.6076,  ...,  0.7558, -0.0126, -1.5131],\n",
              "                                 [-0.2230,  0.5867, -0.7267,  ...,  0.8867, -0.1216, -1.8096]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.6766,  0.6220,  1.5134,  ..., -0.1829,  0.0215,  1.1120],\n",
              "                                 [-0.8275,  0.6928,  1.6118,  ..., -0.2364,  0.0025,  1.4436]],\n",
              "                       \n",
              "                                [[ 1.1364,  0.0790, -0.3696,  ...,  0.2541,  0.9125, -0.5037],\n",
              "                                 [ 1.5442,  0.0934, -0.4613,  ...,  0.0754,  0.0034, -0.9217]],\n",
              "                       \n",
              "                                [[ 1.2640, -0.4548, -0.5475,  ...,  0.5674, -0.4214, -1.0081],\n",
              "                                 [ 1.2149, -0.7560, -0.2841,  ...,  0.4172, -0.3426, -0.9019]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-0.0386, -0.1903,  0.1470,  ..., -0.3462, -0.1094, -0.1254],\n",
              "                                 [-0.0886,  0.0395,  0.0272,  ...,  0.0271, -0.3885,  0.0461]],\n",
              "                       \n",
              "                                [[ 0.1720, -0.3733, -0.0356,  ...,  0.0848,  0.4297,  0.5137],\n",
              "                                 [ 0.2865, -0.2505, -0.2307,  ..., -0.2456,  0.3889,  0.3922]],\n",
              "                       \n",
              "                                [[-0.5288, -0.7186,  0.3537,  ..., -0.7664,  0.2124,  0.3075],\n",
              "                                 [-0.3642, -0.0657,  0.0399,  ..., -0.5451,  0.2110,  0.1761]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.1834, -0.7011,  0.8361,  ...,  0.3994, -0.3654, -0.3145],\n",
              "                                 [ 0.2181, -0.7129,  0.8148,  ...,  0.1566,  0.0218,  0.0515]],\n",
              "                       \n",
              "                                [[-0.3867, -0.7703, -0.4015,  ..., -0.3819, -0.8259,  0.0101],\n",
              "                                 [ 0.2124,  0.1236, -0.8350,  ...,  0.3556, -1.1067, -0.0330]],\n",
              "                       \n",
              "                                [[-0.1518, -0.0204,  0.3507,  ..., -0.0252, -0.2174,  0.3534],\n",
              "                                 [ 0.2348, -0.1401,  0.4585,  ...,  0.2866, -0.0423,  0.0067]]]],\n",
              "                              grad_fn=<CopyBackwards>)),\n",
              "                      (tensor([[[[-0.2997,  1.0674, -0.2439,  ..., -0.2289, -0.6333, -0.6078],\n",
              "                                 [ 0.9438, -1.8757, -0.5755,  ...,  0.1229,  1.7929, -0.3130]],\n",
              "                       \n",
              "                                [[ 0.3514,  1.5137,  0.4524,  ...,  1.1948, -1.3977,  0.3308],\n",
              "                                 [ 0.1387, -1.1616, -0.6226,  ..., -2.1536, -0.1766, -1.1653]],\n",
              "                       \n",
              "                                [[-1.3330, -0.6003,  0.2709,  ...,  1.1001, -0.1418, -0.6230],\n",
              "                                 [-1.5638,  1.9531, -0.2184,  ..., -0.3042, -2.2814,  1.5291]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.4103, -0.2594, -0.7356,  ...,  0.4831,  0.0690,  0.3132],\n",
              "                                 [-0.5332,  0.1774,  1.8028,  ...,  0.1322,  0.1915, -0.9951]],\n",
              "                       \n",
              "                                [[-0.0831, -0.3862, -0.7452,  ..., -0.2725, -0.0534, -0.2660],\n",
              "                                 [-1.4382, -1.8134,  2.8000,  ..., -0.8421, -0.6358,  0.1198]],\n",
              "                       \n",
              "                                [[-1.0351, -0.4564, -1.0894,  ...,  0.2800, -0.1461, -0.3104],\n",
              "                                 [ 0.2128, -0.7712, -0.1874,  ..., -0.7119, -0.5132, -1.6355]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-3.3177e-02, -1.8028e-04,  4.5838e-04,  ...,  1.3248e-02,\n",
              "                                   1.9023e-02,  3.2562e-02],\n",
              "                                 [-3.2168e-02, -1.5418e-01,  1.4814e-01,  ..., -2.6376e-01,\n",
              "                                   1.0807e-02, -6.4121e-01]],\n",
              "                       \n",
              "                                [[-1.0983e-02, -2.1240e-02, -4.1562e-02,  ..., -2.0584e-02,\n",
              "                                  -5.9072e-02, -6.3754e-02],\n",
              "                                 [-7.4979e-01,  4.7896e-01,  3.1803e-01,  ..., -2.9700e-01,\n",
              "                                  -1.4955e-01, -2.6639e-01]],\n",
              "                       \n",
              "                                [[ 5.7588e-02,  8.7107e-02, -3.4391e-02,  ..., -1.3255e-02,\n",
              "                                   2.8237e-02,  1.2374e-01],\n",
              "                                 [-6.9403e-01,  5.8305e-01, -4.0446e-03,  ...,  2.2897e-02,\n",
              "                                  -6.2816e-01,  3.7408e-01]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-3.4895e-02,  7.3233e-03,  3.9967e-02,  ...,  4.8722e-02,\n",
              "                                  -6.5669e-02, -5.9926e-02],\n",
              "                                 [-1.0827e+00, -5.7699e-01,  4.4333e-01,  ...,  2.7596e-01,\n",
              "                                   9.1006e-01, -4.2223e-01]],\n",
              "                       \n",
              "                                [[-2.7116e-02,  1.4112e-02, -1.0806e-01,  ..., -2.1597e-02,\n",
              "                                   1.6286e-02,  6.7958e-03],\n",
              "                                 [ 2.9614e-01,  6.9683e-01,  2.1899e-01,  ...,  1.0726e-02,\n",
              "                                   3.8513e-02,  1.0382e-02]],\n",
              "                       \n",
              "                                [[ 5.4608e-03,  4.4768e-02, -1.2635e-02,  ...,  1.2815e-03,\n",
              "                                   3.0699e-02, -1.0643e-01],\n",
              "                                 [-3.3885e-01, -4.5145e-02,  4.8048e-02,  ..., -4.6988e-01,\n",
              "                                   2.6005e-01, -7.2162e-01]]]], grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 1.0374,  0.3909, -0.6754,  ...,  0.2131, -0.7792,  0.1073],\n",
              "                                 [ 0.6857,  0.6310, -0.8816,  ..., -0.0665,  0.0525, -0.0755]],\n",
              "                       \n",
              "                                [[ 0.5946, -0.2358, -1.0036,  ..., -0.7404, -1.1777, -2.7705],\n",
              "                                 [ 0.9685, -0.8442, -1.0034,  ..., -0.5312, -1.2558, -2.5045]],\n",
              "                       \n",
              "                                [[-0.8795, -0.7718, -1.1451,  ...,  0.3562,  0.6508, -0.3904],\n",
              "                                 [-0.6928, -1.3858, -1.1689,  ...,  0.9893,  0.1329, -0.7229]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-2.6684, -0.8720,  0.7093,  ...,  1.3669,  1.9079,  0.6156],\n",
              "                                 [-3.0017, -0.5988,  0.8993,  ...,  1.0432,  1.9338, -0.0553]],\n",
              "                       \n",
              "                                [[-3.6058,  2.1486,  2.1966,  ...,  1.6174, -2.3318, -0.6465],\n",
              "                                 [-3.9024,  2.2764,  2.3965,  ...,  0.1705, -2.1405, -0.2133]],\n",
              "                       \n",
              "                                [[-0.7480,  1.9170,  1.3470,  ...,  0.4011, -0.4394, -1.1463],\n",
              "                                 [-0.3069,  1.8317,  1.8609,  ...,  0.0209, -0.2684, -0.9492]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.7396, -0.1129,  0.1192,  ..., -0.0200, -0.2694, -0.1733],\n",
              "                                 [ 0.9639, -0.3486, -0.0451,  ..., -0.7172, -0.3156,  0.1609]],\n",
              "                       \n",
              "                                [[ 0.0073, -0.7114, -0.4992,  ..., -0.3623, -0.2582,  0.3008],\n",
              "                                 [ 0.2753, -0.3529, -0.4996,  ..., -0.3469, -0.0021,  0.2831]],\n",
              "                       \n",
              "                                [[ 0.1104,  0.1524, -0.2274,  ..., -0.2116,  0.0372,  0.3177],\n",
              "                                 [ 0.2157,  0.3495, -0.1770,  ...,  0.2640, -0.0096,  0.4679]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.2203, -0.0463,  0.1329,  ...,  0.0979, -0.3664,  0.3431],\n",
              "                                 [-0.0973, -0.2557,  0.4457,  ...,  0.1551, -0.3278,  0.6508]],\n",
              "                       \n",
              "                                [[ 0.5532, -0.3937,  0.4316,  ..., -0.0912,  0.6116, -0.1449],\n",
              "                                 [ 0.6388, -0.4841,  0.4255,  ..., -0.0629,  0.5913, -0.1810]],\n",
              "                       \n",
              "                                [[ 0.4216, -0.3658, -0.3054,  ...,  0.1030,  0.0099, -0.1975],\n",
              "                                 [ 0.3812, -0.3219, -0.4682,  ..., -0.3154,  0.0673, -0.2982]]]],\n",
              "                              grad_fn=<CopyBackwards>)),\n",
              "                      (tensor([[[[-0.0638, -0.3080,  0.1955,  ..., -0.2980,  0.1688,  0.0431],\n",
              "                                 [ 0.5026, -0.5220,  3.4995,  ..., -1.0278,  0.1422,  2.3666]],\n",
              "                       \n",
              "                                [[ 0.4591,  0.1555, -0.1794,  ..., -0.2225, -0.4867,  0.2342],\n",
              "                                 [-0.4296, -0.2702, -0.8925,  ..., -1.1856,  0.8598, -0.2742]],\n",
              "                       \n",
              "                                [[-0.3287, -0.1999, -0.2862,  ..., -0.5182,  0.0972,  0.6838],\n",
              "                                 [ 2.6562,  1.6259, -1.6430,  ...,  0.5375,  1.0974, -3.0638]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.1022, -0.1004,  0.0337,  ..., -0.4126,  0.0917,  0.2785],\n",
              "                                 [-0.3013,  0.2233, -0.8632,  ..., -2.7654,  0.0675,  2.4467]],\n",
              "                       \n",
              "                                [[ 0.3300,  0.2506,  0.0510,  ..., -0.0276, -0.1510, -0.1070],\n",
              "                                 [-3.6227,  0.0169, -1.4157,  ..., -0.7226, -1.0749,  1.1248]],\n",
              "                       \n",
              "                                [[-0.0671, -0.4478, -0.0450,  ..., -0.1774, -0.1607, -0.0955],\n",
              "                                 [-4.6108,  0.6886,  0.8102,  ..., -0.4719,  0.8873, -2.4897]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[-0.0404, -0.0917,  0.0642,  ..., -0.0035, -0.0486,  0.0240],\n",
              "                                 [-0.2731,  0.0407,  0.5085,  ..., -0.1216, -0.2275, -0.7928]],\n",
              "                       \n",
              "                                [[ 0.0031, -0.1010,  0.0053,  ...,  0.0069, -0.0100,  0.0144],\n",
              "                                 [-0.1159, -0.6017,  0.1824,  ..., -0.1040,  0.0132,  0.0196]],\n",
              "                       \n",
              "                                [[ 0.1149, -0.0396, -0.0644,  ..., -0.1404,  0.0345,  0.0140],\n",
              "                                 [ 0.4707,  0.5479,  0.1769,  ..., -1.8493, -0.6008, -0.0666]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[ 0.0385, -0.0342, -0.0351,  ...,  0.0089, -0.0578,  0.0940],\n",
              "                                 [ 0.3749,  0.0586, -0.0326,  ..., -0.1291, -0.1929,  0.6304]],\n",
              "                       \n",
              "                                [[-0.0040,  0.0416,  0.0358,  ...,  0.0166,  0.0453,  0.0051],\n",
              "                                 [ 0.0923, -0.0818,  0.3751,  ...,  0.5956, -0.0576, -0.0671]],\n",
              "                       \n",
              "                                [[ 0.0988,  0.0028,  0.0458,  ...,  0.1539,  0.0561,  0.0554],\n",
              "                                 [ 0.0944, -0.1632,  0.0327,  ...,  0.9541,  0.3676,  0.2072]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.4540, -0.9974, -2.6459,  ...,  1.1923, -0.3397,  3.7741],\n",
              "                                 [ 0.5595, -0.9436, -1.4177,  ...,  0.9573, -0.9510,  3.5285]],\n",
              "                       \n",
              "                                [[ 0.5767, -1.4281, -0.0372,  ...,  0.2962,  1.1243,  1.1418],\n",
              "                                 [-0.3780, -1.4567,  1.4320,  ..., -0.2319,  0.4736,  0.6123]],\n",
              "                       \n",
              "                                [[ 0.4139, -1.4614, -0.8103,  ..., -3.3714,  0.1478,  0.2090],\n",
              "                                 [-0.1716, -1.2067, -0.2029,  ..., -2.4051, -0.5142,  0.5125]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-3.4578, -0.9712,  0.9847,  ..., -0.2011, -1.6939,  0.2227],\n",
              "                                 [-1.9282, -0.9421,  0.6625,  ..., -0.3203, -2.4944,  0.4157]],\n",
              "                       \n",
              "                                [[-1.9077,  2.6054,  0.7046,  ...,  3.3153,  0.8114, -1.1166],\n",
              "                                 [-0.5560,  2.4935,  1.0945,  ...,  3.5782,  0.8835, -1.7877]],\n",
              "                       \n",
              "                                [[-0.7894, -0.6044, -0.4530,  ...,  2.1209, -1.0290,  1.3274],\n",
              "                                 [-1.5356,  0.6271, -1.3167,  ...,  1.9269, -1.3602,  2.0155]]]],\n",
              "                              grad_fn=<CopyBackwards>),\n",
              "                       tensor([[[[ 0.2468,  0.4531, -0.2456,  ..., -0.2639, -0.4747,  0.2349],\n",
              "                                 [ 0.6563,  0.5046, -0.2421,  ...,  0.0345, -0.6680,  0.2661]],\n",
              "                       \n",
              "                                [[ 0.1554,  0.0104,  0.0888,  ...,  0.5652,  0.1155,  0.2966],\n",
              "                                 [ 0.2705, -0.4564,  0.2788,  ...,  0.3807,  0.1667,  0.0607]],\n",
              "                       \n",
              "                                [[-0.5218, -0.1890,  0.0533,  ..., -0.1139,  0.2080, -0.0636],\n",
              "                                 [-0.3670, -0.1278, -0.1607,  ...,  0.1446, -0.0988, -0.1946]],\n",
              "                       \n",
              "                                ...,\n",
              "                       \n",
              "                                [[-0.0757,  0.8512,  0.1290,  ...,  0.5617,  0.4623, -0.4476],\n",
              "                                 [-0.1185,  0.9600,  0.4644,  ...,  0.6976,  0.4706,  0.1709]],\n",
              "                       \n",
              "                                [[ 0.0076, -0.1633,  0.0613,  ..., -0.4322, -0.1017,  0.0051],\n",
              "                                 [ 0.0926, -0.3790,  0.3583,  ..., -0.1947, -0.1139, -0.2700]],\n",
              "                       \n",
              "                                [[ 0.8125,  0.0627,  0.4241,  ...,  0.3436,  0.0131, -0.7254],\n",
              "                                 [ 0.4459,  0.2707,  0.4448,  ...,  0.2226,  0.4030, -0.7320]]]],\n",
              "                              grad_fn=<CopyBackwards>)))),\n",
              "                    ('encoder_last_hidden_state',\n",
              "                     tensor([[[ 0.4624, -0.2475,  0.0902,  ...,  0.1127,  0.6529,  0.2203],\n",
              "                              [ 0.4538, -0.2948,  0.2556,  ..., -0.0442,  0.6858,  0.4372]]],\n",
              "                            grad_fn=<NativeLayerNormBackward>))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHjJ-f62JcKr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}